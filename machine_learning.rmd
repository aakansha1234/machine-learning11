# Practical Machine Learning - Final Project

## Overview

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


### Goal

The goal of your project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

### Overview of the project

In this document we will gather the data and clean it from missing values and useless predictors. The training dataset is split in a Training and a Validation datatasets in order to measure the out of sample errors of the predictive models. The Training datasets is used to build both Random Forest and Generalized Boosted Regression Models, which are then compared


## Libraries

Loading the required packages

```{r libraries, message=FALSE}
library(caret)
library(randomForest)
library(gbm)
```


## Downloading the data and reading it into R

The data is taken from http://groupware.les.inf.puc-rio.br/har

```{r download, eval = FALSE, message=FALSE}
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
```
```{r read}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```


## Cleaning the training dataset and splitting into Training and Validation datasets

Since many of the columns are composed almost entirely of missing values, we will remove those columns. Columns not relevant to accelerometer measurements are also removed. 

```{r clean}
## Remove columns with NA missing values
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0] 

## Remove predictors that are not relevant to accelerometer measurements.
classe <- training$classe
trainRm <- grepl("^X|timestamp|window", names(training))
training <- training[, !trainRm]
trainClean <- training[, sapply(training, is.numeric)]
trainClean$classe <- classe
testRm <- grepl("^X|timestamp|window", names(testing))
testing <- testing[, !testRm]
testClean <- testing[, sapply(testing, is.numeric)]
```

The cleaned training dataset is split into Training and Validation datasets.

```{r train validation}
set.seed(2431)
inTrain <- createDataPartition(trainClean$classe, p=0.6, list=FALSE)
trTraining <- trainClean[inTrain, ]
trValidation <- trainClean[-inTrain, ]
```


## Modeling with Random Forest

The first model to be tested is the Random Forest model.

```{r random forest model}
set.seed(573)
rfModel<- train(classe ~., data=trTraining, method="rf", trControl=trainControl(method="cv", 5), importance=TRUE, ntree=100)
rfModel
```

Accuracy of Random Forest Model

```{r confmatrixrf}
predictRfModel<- predict(rfModel, trTraining)
confusionMatrix(trTraining$classe, predictRfModel)$overall[1]
```

## Modeling with Generalized Boost

```{r gbm}
set.seed(24562)
gbmModel1 <- train(classe ~ ., data=trTraining, method = "gbm", trControl = trainControl(method = "repeatedcv",
                                                                                         number = 5,
                                                                                         repeats = 1),
                   verbose = FALSE)
gbmModel <- gbmModel1$finalModel
head(summary(gbmModel)) ## Shows the highest influence predictors
```

Accuracy of the GBM model

```{r confmatrixgbm}
predictGbmModel<- predict(gbmModel1, trTraining)
confusionMatrix(trTraining$classe, predictGbmModel)$overall[1]
```

```{r plotgbm}
plot(gbmModel1, ylim=c(0.74, 1))
```

## Calculating the out of sample error for the models

To calculate the out of sample error, we use both models to predict the classe variable on the Validation dataset, and create a table to show how many of those predictions were correct (confusion matrix).

```{r predictions}
predRF <- predict(rfModel, trValidation)
predGBM <- predict(gbmModel1, trValidation)
combDF <- data.frame(predRF, predGBM, classe = trValidation$classe)
combModel <- train(classe~., data=combDF, method="rf")
```

### RF out of sample error

```{r rfoos}
tbRF <- table(combDF[,c(1,3)])
tbRF
RFoosError <- 1 - sum(diag(tbRF))/sum(tbRF)
RFoosError
```

### GBM out of sample error
```{r gbmoos}
tbGBM <- table(combDF[,c(2,3)])
tbGBM
GBMoosError <- 1 - sum(diag(tbGBM))/sum(tbGBM)
GBMoosError
```

From the accuracy and the out of sample error calculations, it appears clear that the RF model is better. Thus this model will be used for the predictions on the testing dataset.

### Predicting with RF on the actual testing set

```{r testingpredict}
predRFtest <- predict(rfModel, testClean)
predRFtest
```
